{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd0bd4468f27b846f66ffb6ca38a5541ac9982a9c60ec61339481ff865c58c65a1c",
   "display_name": "Python 3.8.10 64-bit ('maRLio': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "bd4468f27b846f66ffb6ca38a5541ac9982a9c60ec61339481ff865c58c65a1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<h1>Information</h1>\n",
    "\n",
    "<h3>Paper used as outline & insperation</h3>\n",
    "https://towardsdatascience.com/explained-curiosity-driven-learning-in-rl-exploration-by-random-network-distillation-72b18e69eb1b"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[name: \"/device:CPU:0\"\ndevice_type: \"CPU\"\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 8730353498045840482\n, name: \"/device:XLA_CPU:0\"\ndevice_type: \"XLA_CPU\"\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 2203243613317093611\nphysical_device_desc: \"device: XLA_CPU device\"\n, name: \"/device:GPU:0\"\ndevice_type: \"GPU\"\nmemory_limit: 4940566368\nlocality {\n  bus_id: 1\n  links {\n  }\n}\nincarnation: 15627421989151166618\nphysical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:06:00.0, compute capability: 7.5\"\n, name: \"/device:XLA_GPU:0\"\ndevice_type: \"XLA_GPU\"\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 18096483059872053751\nphysical_device_desc: \"device: XLA_GPU device\"\n]\n"
     ]
    }
   ],
   "source": [
    "#? imports\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
    "\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Flatten, Reshape, GRU, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# constant seed for tensorflow and numpy\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? initialize the environment and envrionment variables\n",
    "ACTION_SPACE = SIMPLE_MOVEMENT\n",
    "ENVIRONMENT = 'SuperMarioBros-v3'\n",
    "\n",
    "env = gym_super_mario_bros.make(ENVIRONMENT)\n",
    "env = JoypadSpace(env, ACTION_SPACE)\n",
    "\n",
    "OBSERVATION_SPACE = env.observation_space.sample().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? agent static vars\n",
    "CURIOSITY_EMBEDDING = 128\n",
    "\n",
    "EPOCHS=10\n",
    "BATCH_SIZE=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? build the target and predictor networks\n",
    "def build_curiosity_network(trainable):\n",
    "    i = Input(OBSERVATION_SPACE)\n",
    "\n",
    "    x = Conv2D(4, (5, 5), activation='relu', trainable=trainable)(i)\n",
    "    x = MaxPool2D((4, 4))(x)\n",
    "\n",
    "    x = Conv2D(4, (5, 5), activation='relu', trainable=trainable)(x)\n",
    "    x = MaxPool2D((4, 4))(x)\n",
    "\n",
    "    x = Conv2D(8, (5, 5), activation='relu', trainable=trainable)(x)\n",
    "    x = MaxPool2D((4, 4))(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(CURIOSITY_EMBEDDING, activation='relu', trainable=trainable)(x)\n",
    "\n",
    "    model = Model(i, x)\n",
    "\n",
    "    return model\n",
    "\n",
    "target_model = build_curiosity_network(False)\n",
    "predictor_model = build_curiosity_network(True)\n",
    "\n",
    "predictor_model.compile(loss='mse', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? build the actor network\n",
    "i = Input(OBSERVATION_SPACE)\n",
    "\n",
    "x = Conv2D(4, (8, 8), padding='same', activation='relu')(i)\n",
    "x = MaxPool2D((4, 4))(x)\n",
    "\n",
    "x = Conv2D(8, (6, 6), padding='same', activation='relu')(x)\n",
    "x = MaxPool2D()(x)\n",
    "\n",
    "x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
    "x = MaxPool2D()(x)\n",
    "\n",
    "x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "x = MaxPool2D()(x)\n",
    "\n",
    "# x = Reshape((-1, 1))(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "# x = GRU(128)(x)\n",
    "x = Dense(len(ACTION_SPACE))(x)\n",
    "\n",
    "actor_model = Model(i, x)\n",
    "actor_model.compile(loss='mse', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? build the agent\n",
    "class Agent:\n",
    "    def __init__(self, target_model, predictor_model, actor_model, batch_size=32, Y=0.9, epsilon=2, epsilon_decay=0.998, epsilon_min=0.2, action_space=len(ACTION_SPACE), min_memory_size=1000, memory_length=100000):\n",
    "\n",
    "        # the target model acts as a hash function, creating a unique, unknown output for any observation\n",
    "        self.target = target_model\n",
    "        # the predictor tries to match the output of the target.  This means that its error can act as a function for how 'familiar' an observation is\n",
    "        self.predictor = predictor_model\n",
    "\n",
    "        # model that actually selects what action to do\n",
    "        self.actor = actor_model\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.Y = Y\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.min_memory_size = min_memory_size\n",
    "        self.memory = deque(maxlen=memory_length)\n",
    "\n",
    "    def add_experience(self, observation, action, new_observation):\n",
    "        self.memory.append([\n",
    "            observation,\n",
    "            action,\n",
    "            new_observation\n",
    "        ])\n",
    "\n",
    "    def act(self, obs):\n",
    "\n",
    "        # randomly explore or take predicted action\n",
    "        if (random.uniform(0, 1) < self.epsilon):\n",
    "            return (0, random.randint(0, self.action_space-1), self.action_space-1)\n",
    "        else:\n",
    "            prediction = self.actor.predict(np.asarray([obs]))\n",
    "\n",
    "            return (1, prediction.argmax() - 1, prediction)\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # extra info from the memory\n",
    "        observations     = np.array([x[0] for x in list(self.memory)])\n",
    "        actions          = np.array([x[1] for x in list(self.memory)])\n",
    "        next_observation = np.array([x[2] for x in list(self.memory)])\n",
    "\n",
    "        # calculate the novelty of each observation\n",
    "        y_true = self.target.predict(observations)\n",
    "        y_pred = self.predictor.predict(observations)\n",
    "\n",
    "        novelty = np.mean(np.square(y_true - y_pred), axis=1)\n",
    "\n",
    "        # train the predictor\n",
    "        self.predictor.fit(observations, y_true, batch_size=self.batch_size, verbose=0)\n",
    "\n",
    "        # don't the actor train unless there is sufficient memory\n",
    "        if (len(self.memory) < self.min_memory_size):\n",
    "            return np.mean(novelty)\n",
    "\n",
    "        # calculate the new q values for the actor\n",
    "        current_qs = self.actor.predict(observations)\n",
    "\n",
    "        max_future_reward = self.actor.predict(next_observation).max()\n",
    "\n",
    "        new_qs = (1 - self.Y) * novelty + self.Y * max_future_reward\n",
    "\n",
    "        # update the q values with new qs calculated with the novelty and the estimated maximum future novelty\n",
    "        for i in range(len(new_qs)):\n",
    "            current_qs[i][actions[i]] = new_qs[i]\n",
    "\n",
    "        # train the actor\n",
    "        self.actor.fit(observations, current_qs, batch_size=self.batch_size, verbose=0)\n",
    "\n",
    "        # increment the epsilon value\n",
    "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "\n",
    "        return np.mean(novelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training | Epoch time: 7s | Actor: False | Epsilon: 1.2 | Mean Novelty: 1586\n",
      "Training | Epoch time: 7s | Actor: False | Epsilon: 1.2 | Mean Novelty: 600\n",
      "Training | Epoch time: 9s | Actor: False | Epsilon: 1.2 | Mean Novelty: 563\n",
      "Training | Epoch time: 11s | Actor: False | Epsilon: 1.2 | Mean Novelty: 603\n",
      "Training | Epoch time: 23s | Actor: True | Epsilon: 1.13 | Mean Novelty: 657\n",
      "Training | Epoch time: 25s | Actor: True | Epsilon: 1.08 | Mean Novelty: 640\n",
      "Training | Epoch time: 29s | Actor: True | Epsilon: 1.02 | Mean Novelty: 600\n",
      "Training | Epoch time: 33s | Actor: True | Epsilon: 0.97 | Mean Novelty: 591\n",
      "Training | Epoch time: 40s | Actor: True | Epsilon: 0.92 | Mean Novelty: 549\n",
      "Training | Epoch time: 45s | Actor: True | Epsilon: 0.88 | Mean Novelty: 593\n",
      "Training | Epoch time: 49s | Actor: True | Epsilon: 0.83 | Mean Novelty: 569\n",
      "Training | Epoch time: 52s | Actor: True | Epsilon: 0.79 | Mean Novelty: 561\n",
      "Training | Epoch time: 56s | Actor: True | Epsilon: 0.75 | Mean Novelty: 556\n",
      "Training | Epoch time: 57s | Actor: True | Epsilon: 0.71 | Mean Novelty: 517\n",
      "Training | Epoch time: 49s | Actor: True | Epsilon: 0.68 | Mean Novelty: 617\n",
      "Training | Epoch time: 63s | Actor: True | Epsilon: 0.64 | Mean Novelty: 661\n",
      "Training | Epoch time: 67s | Actor: True | Epsilon: 0.61 | Mean Novelty: 574\n",
      "Training | Epoch time: 66s | Actor: True | Epsilon: 0.58 | Mean Novelty: 590\n",
      "Training | Epoch time: 67s | Actor: True | Epsilon: 0.55 | Mean Novelty: 550\n",
      "Training | Epoch time: 56s | Actor: True | Epsilon: 0.52 | Mean Novelty: 628\n",
      "Training | Epoch time: 69s | Actor: True | Epsilon: 0.5 | Mean Novelty: 613\n",
      "Training | Epoch time: 71s | Actor: True | Epsilon: 0.47 | Mean Novelty: 614\n",
      "Training | Epoch time: 73s | Actor: True | Epsilon: 0.45 | Mean Novelty: 632\n",
      "Training | Epoch time: 73s | Actor: True | Epsilon: 0.43 | Mean Novelty: 560\n",
      "Training | Epoch time: 44s | Actor: True | Epsilon: 0.4 | Mean Novelty: 582\n",
      "Training | Epoch time: 77s | Actor: True | Epsilon: 0.38 | Mean Novelty: 579\n",
      "Training | Epoch time: 79s | Actor: True | Epsilon: 0.36 | Mean Novelty: 549\n",
      "Training | Epoch time: 80s | Actor: True | Epsilon: 0.35 | Mean Novelty: 553\n",
      "Training | Epoch time: 81s | Actor: True | Epsilon: 0.33 | Mean Novelty: 531\n",
      "Training | Epoch time: 61s | Actor: True | Epsilon: 0.31 | Mean Novelty: 576\n",
      "Training | Epoch time: 83s | Actor: True | Epsilon: 0.3 | Mean Novelty: 490\n",
      "Training | Epoch time: 49s | Actor: True | Epsilon: 0.28 | Mean Novelty: 588\n",
      "Training | Epoch time: 44s | Actor: True | Epsilon: 0.27 | Mean Novelty: 631\n",
      "Training | Epoch time: 90s | Actor: True | Epsilon: 0.25 | Mean Novelty: 541\n",
      "Training | Epoch time: 89s | Actor: True | Epsilon: 0.24 | Mean Novelty: 599\n",
      "Training | Epoch time: 89s | Actor: True | Epsilon: 0.23 | Mean Novelty: 630\n",
      "Training | Epoch time: 87s | Actor: True | Epsilon: 0.22 | Mean Novelty: 593\n",
      "Training | Epoch time: 88s | Actor: True | Epsilon: 0.2 | Mean Novelty: 575\n",
      "Training | Epoch time: 89s | Actor: True | Epsilon: 0.2 | Mean Novelty: 603\n",
      "Training | Epoch time: 90s | Actor: True | Epsilon: 0.2 | Mean Novelty: 580\n",
      "Training | Epoch time: 76s | Actor: True | Epsilon: 0.2 | Mean Novelty: 606\n",
      "Training | Epoch time: 90s | Actor: True | Epsilon: 0.2 | Mean Novelty: 584\n",
      "Training | Epoch time: 90s | Actor: True | Epsilon: 0.2 | Mean Novelty: 612\n",
      "Training | Epoch time: 90s | Actor: True | Epsilon: 0.2 | Mean Novelty: 587\n",
      "Training | Epoch time: 89s | Actor: True | Epsilon: 0.2 | Mean Novelty: 559\n",
      "Training | Epoch time: 89s | Actor: True | Epsilon: 0.2 | Mean Novelty: 608\n",
      "Training | Epoch time: 90s | Actor: True | Epsilon: 0.2 | Mean Novelty: 501\n",
      "Training | Epoch time: 90s | Actor: True | Epsilon: 0.2 | Mean Novelty: 691\n",
      "Training | Epoch time: 90s | Actor: True | Epsilon: 0.2 | Mean Novelty: 578\n",
      "Training | Epoch time: 90s | Actor: True | Epsilon: 0.2 | Mean Novelty: 580\n",
      "Training | Epoch time: 45s | Actor: True | Epsilon: 0.2 | Mean Novelty: 579\n",
      "(240, 256, 3)\n",
      "(1, 7)\n",
      "(240, 256, 3)\n",
      "(1, 7)\n"
     ]
    }
   ],
   "source": [
    "#? begin the training process\n",
    "agent = Agent(target_model, predictor_model, actor_model, min_memory_size=10000, memory_length=20000, epsilon=1.2, epsilon_decay=0.95)\n",
    "\n",
    "j = 0\n",
    "for i in range(1, EPOCHS+1):\n",
    "    obs = env.reset()\n",
    "    t = datetime.datetime.now()\n",
    "\n",
    "    while not env.done:\n",
    "        # take an action\n",
    "        action = agent.act(obs)\n",
    "        try:\n",
    "            new_obs, reward, done, info = env.step(action[1])\n",
    "        except:\n",
    "            print(obs.shape)\n",
    "            print(action[2].shape)\n",
    "            break\n",
    "\n",
    "        if j % 20 == 0:\n",
    "            env.render()\n",
    "\n",
    "        # add the experience to the agent's memory\n",
    "        agent.add_experience(obs, action[1], new_obs)\n",
    "\n",
    "        obs = new_obs\n",
    "\n",
    "        j += 1\n",
    "        if j % 2000 == 0:\n",
    "            # train the agent on its past experiences\n",
    "            mean_novelty = agent.train()\n",
    "\n",
    "            time = (int)((datetime.datetime.now() - t).total_seconds())\n",
    "            actor = len(agent.memory) >= agent.min_memory_size\n",
    "            eps = (int)(agent.epsilon * 100) / 100\n",
    "\n",
    "            print(f'Training | Epoch time: {time}s | Actor: {actor} | Epsilon: {eps} | Mean Novelty: {(int)(mean_novelty)}')\n",
    "            t = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}