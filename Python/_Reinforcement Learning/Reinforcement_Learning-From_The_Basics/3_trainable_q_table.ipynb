{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python395jvsc74a57bd086c080258fc1895424e842bd7334418d491c3ba45b63b72b9209632f640ad29e",
   "display_name": "Python 3.9.5 64-bit ('rl': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "cadee6d9ca5a51ce1691b807cfbce535d2a09246bd6445805893caa7c00cfe39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? imports\n",
    "import gym\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? initialize the environment\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "# get the action and observation space (used when constructing the q table)\n",
    "ACTION_SPACE      = env.action_space.n\n",
    "OBSERVATION_SPACE = len(env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? build the q table\n",
    "Q_INCREMENTS = 20 # how detailed the q table is\n",
    "DISCRETE_OS_SIZE = [Q_INCREMENTS] * OBSERVATION_SPACE\n",
    "\n",
    "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [ACTION_SPACE]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? build a function that takes an observation and return the action given by the q table\n",
    "def obs_To_Index(env, obs, increments):\n",
    "\n",
    "    # get the bounds of the observation_space\n",
    "    obs_min = env.observation_space.low\n",
    "    obs_max = env.observation_space.high\n",
    "\n",
    "    # normalize the observation\n",
    "    obs = (obs - obs_min) / (obs_max - obs_min)\n",
    "\n",
    "    # convert the normalized array to an integer indice\n",
    "    indice = tuple(np.floor(obs * increments).astype(int))\n",
    "\n",
    "    return indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? initialize parameters related to training\n",
    "EPOCHS        = 1000  # number of environments to simulate\n",
    "DISCOUNT      = 0.95  # how much the agent cares about future rewards\n",
    "LEARNING_RATE = 0.1   # how quickly values in the q table change\n",
    "\n",
    "RENDER_EVERY  = 500   # how often to render a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Won at epoch 488\n",
      "Won at epoch 633\n",
      "Won at epoch 736\n",
      "Won at epoch 788\n",
      "Won at epoch 804\n",
      "Won at epoch 824\n",
      "Won at epoch 826\n",
      "Won at epoch 828\n",
      "Won at epoch 829\n",
      "Won at epoch 830\n",
      "Won at epoch 835\n",
      "Won at epoch 836\n",
      "Won at epoch 837\n",
      "Won at epoch 841\n",
      "Won at epoch 842\n",
      "Won at epoch 843\n",
      "Won at epoch 844\n",
      "Won at epoch 845\n",
      "Won at epoch 846\n",
      "Won at epoch 848\n",
      "Won at epoch 849\n",
      "Won at epoch 851\n",
      "Won at epoch 853\n",
      "Won at epoch 854\n",
      "Won at epoch 860\n",
      "Won at epoch 861\n",
      "Won at epoch 862\n",
      "Won at epoch 864\n",
      "Won at epoch 865\n",
      "Won at epoch 866\n",
      "Won at epoch 867\n",
      "Won at epoch 869\n",
      "Won at epoch 870\n",
      "Won at epoch 871\n",
      "Won at epoch 872\n",
      "Won at epoch 873\n",
      "Won at epoch 903\n",
      "Won at epoch 906\n",
      "Won at epoch 909\n",
      "Won at epoch 910\n",
      "Won at epoch 912\n",
      "Won at epoch 917\n",
      "Won at epoch 918\n",
      "Won at epoch 919\n",
      "Won at epoch 921\n",
      "Won at epoch 922\n",
      "Won at epoch 992\n",
      "Won at epoch 993\n",
      "Won at epoch 995\n",
      "Won at epoch 1000\n"
     ]
    }
   ],
   "source": [
    "#? train the agent by updating the q table\n",
    "for e in range(1, EPOCHS+1):\n",
    "\n",
    "    # store the initial state of the environment\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "        \n",
    "    while not done:\n",
    "\n",
    "        # render every [RENDER_EVERY] epochs\n",
    "        if e % RENDER_EVERY == 0:\n",
    "            env.render()\n",
    "\n",
    "        # find the discrete cell coresponding to the current observation\n",
    "        indice = obs_To_Index(env, observation, Q_INCREMENTS)\n",
    "\n",
    "        # select the action to take\n",
    "        action = q_table[indice].argmax()\n",
    "\n",
    "        # take the action\n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "\n",
    "        # calculate the predicted future reward\n",
    "        new_indice = obs_To_Index(env, new_observation, Q_INCREMENTS)\n",
    "        future_reward = reward + DISCOUNT * q_table[new_indice].max()\n",
    "\n",
    "        # update the value in the q table\n",
    "        current_q = q_table[indice + (action,)]\n",
    "        new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * future_reward\n",
    "\n",
    "        q_table[indice + (action,)] = new_q\n",
    "\n",
    "        # update the current observation\n",
    "        observation = new_observation\n",
    "\n",
    "        if (observation[0] >= env.goal_position):\n",
    "            print(f'Won on epoch {e}')\n",
    "\n",
    "    # debug message\n",
    "    if e % 50 == 0:\n",
    "        print(f'Reached epoch {e}')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}