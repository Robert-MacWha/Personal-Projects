{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python395jvsc74a57bd09c95df90105059b895687d1f25fd737b41c4c337fb792ba907f3948471b501aa",
   "display_name": "Python 3.9.5 64-bit ('gym': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "cadee6d9ca5a51ce1691b807cfbce535d2a09246bd6445805893caa7c00cfe39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? imports\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? initialize the environment\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "# get the action and observation space (used when constructing the q table)\n",
    "ACTION_SPACE      = env.action_space.n\n",
    "OBSERVATION_SPACE = len(env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"functional_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 2)]               0         \n_________________________________________________________________\ndense (Dense)                (None, 16)                48        \n_________________________________________________________________\ndense_1 (Dense)              (None, 16)                272       \n_________________________________________________________________\ndense_2 (Dense)              (None, 3)                 51        \n=================================================================\nTotal params: 371\nTrainable params: 371\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
     ]
    }
   ],
   "source": [
    "#? build the model\n",
    "\n",
    "inp = Input(shape=(OBSERVATION_SPACE))\n",
    "\n",
    "hidden = Dense(16, activation='relu')(inp)\n",
    "hidden = Dense(16, activation='relu')(hidden)\n",
    "\n",
    "out  = Dense(ACTION_SPACE, activation='linear')(hidden)\n",
    "\n",
    "model = Model(inp, out)\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? initialize parameters related to training\n",
    "EPOCHS        = 5000    # number of environments to simulate\n",
    "DISCOUNT      = 0.95    # how much the agent cares about future rewards\n",
    "EPSILON       = 0.5     # chance of the agent taking a random action\n",
    "EPSILON_DECAY = 0.9998\n",
    "\n",
    "RENDER_EVERY  = 10      # how often to render a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? initialize the memory array\n",
    "memory = []\n",
    "\n",
    "BATCH_SIZE = 32         # minimum number of samples required to train the model\n",
    "MAX_MEMORY_SIZE = 1000  # maximum number of states to store in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[-0.12809888  0.04532868  0.07571742]]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-39a07eb0a10d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# calculate the correct output value for the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mnew_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mnew_prediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfuture_reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# add the data required for training to the memory array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "#? train the agent\n",
    "for e in range(1, EPOCHS+1):\n",
    "\n",
    "    # store the initial state of the environment\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "        \n",
    "    while not done:\n",
    "\n",
    "        # render every [RENDER_EVERY] epochs\n",
    "        if e % RENDER_EVERY == 0:\n",
    "            env.render()\n",
    "\n",
    "        prediction = model.predict(np.asarray([observation]))[0] # predicted reward for each action\n",
    "\n",
    "        # select the action to take\n",
    "        if random.uniform(0, 1) < EPSILON:\n",
    "            action = env.action_space.sample()                    # random action (exploration)\n",
    "        else:\n",
    "            action = prediction.argmax()\n",
    "\n",
    "        # take the action\n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "\n",
    "        # calculate the predicted future reward\n",
    "        next_prediction = model.predict(np.asarray([new_observation]))[0]\n",
    "        next_reward     = next_prediction.max()\n",
    "        future_reward   = reward + DISCOUNT * next_reward\n",
    "\n",
    "        # calculate the correct output value for the model\n",
    "        new_prediction = prediction\n",
    "        new_prediction[action] = future_reward\n",
    "\n",
    "        # add the data required for training to the memory array\n",
    "        memory.append([\n",
    "            observation,\n",
    "            new_prediction\n",
    "        ])\n",
    "\n",
    "        # limit the size of the memory\n",
    "        memory = memory[:MAX_MEMORY_SIZE]\n",
    "\n",
    "        # update the current observation\n",
    "        observation = new_observation\n",
    "\n",
    "        if observation[0] >= env.goal_position:\n",
    "            print(f'Won on epoch {e}')\n",
    "\n",
    "    # reduce epsilon\n",
    "    EPSILON = EPSILON * EPSILON_DECAY\n",
    "\n",
    "    # train the model if there are enough memories\n",
    "    if len(memory) > BATCH_SIZE:\n",
    "\n",
    "        Xs = np.array([m[0] for m in memory])\n",
    "        ys = np.array([m[1] for m in memory])\n",
    "\n",
    "        verbose = 0\n",
    "        if e % 5 == 0:\n",
    "            verbose = 1\n",
    "\n",
    "        model.fit(Xs, ys, batch_size=BATCH_SIZE, verbose=verbose)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# save the model\n",
    "model.save('./5_trained_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}